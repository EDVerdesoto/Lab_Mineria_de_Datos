# SELECCI√ìN DEL MODELO: CODEBERT FINE-TUNED

Se seleccion√≥ **CodeBERT** (modelo transformers pre-entrenado en c√≥digo) como arquitectura principal para la detecci√≥n de vulnerabilidades, con un pipeline de entrenamiento optimizado mediante **cacheo de tokenizaci√≥n** y **entrenamiento por √©pocas con GPU**.

---

## üéØ Por qu√© CodeBERT

### 1. Comprensi√≥n Sem√°ntica de C√≥digo
CodeBERT es un modelo BERT pre-entrenado en 6 lenguajes de programaci√≥n (Python, Java, JavaScript, PHP, Ruby, Go) y documentaci√≥n asociada. A diferencia de modelos tradicionales que tratan el c√≥digo como texto plano:

- **Entiende sintaxis**: Reconoce estructuras como loops, condicionales, llamadas a funciones
- **Captura dependencias**: Relaciona variables con sus usos a trav√©s de m√∫ltiples l√≠neas
- **Contexto bi-direccional**: Analiza c√≥digo antes y despu√©s de cada token simult√°neamente

**Ejemplo:**
```python
# CodeBERT entiende que 'query' es vulnerable porque:
# 1. Se construye con f-string (l√≠nea 2)
# 2. Contiene variables externas (user_input)
# 3. Se pasa a execute() (funci√≥n de DB)
query = f"SELECT * FROM users WHERE id={user_input}"
db.execute(query)  # CWE-89: SQL Injection
```

### 2. Transfer Learning
El pre-entrenamiento en millones de l√≠neas de c√≥digo Open Source permite:
- **Menos datos requeridos**: Fine-tuning con ~500k muestras en lugar de millones
- **Generalizaci√≥n**: Detecta patrones incluso en lenguajes no vistos durante fine-tuning
- **Conocimiento previo**: Ya conoce APIs peligrosas (eval, exec, system, etc.)

### 3. Manejo de C√≥digo Largo (Sliding Window)
CodeBERT tiene l√≠mite de 512 tokens, pero nuestras funciones pueden ser m√°s largas. Soluci√≥n implementada:

```python
# Funci√≥n de 2000 tokens se divide en ventanas superpuestas:
Ventana 1: tokens[0:512]      # 50% overlap
Ventana 2: tokens[256:768]
Ventana 3: tokens[512:1024]
...
# Predicci√≥n final = max(probabilidades_ventanas)
```

**Beneficios:**
- No se pierde informaci√≥n de funciones largas
- STRIDE=256 asegura que patrones en los bordes se capturen
- MAX_WINDOWS=8 previene OOM en GPU

### 4. Manejo de Desbalanceo de Clases

**Problema:** Dataset real tiene distribuci√≥n:
- Safe: 72%
- CWE-79 (XSS): 8%
- CWE-89 (SQLi): 6%
- Otras CWEs: <2% cada una

**Soluciones implementadas:**

#### a) Focal Loss
```python
FL(pt) = -Œ±(1 - pt)^Œ≥ * log(pt)
```
- **Œ≥=2.0**: Reduce peso de ejemplos f√°ciles (Safe bien clasificado)
- **Œ±=class_weights**: Penaliza m√°s errores en clases minoritarias

#### b) WeightedRandomSampler
```python
# En cada √©poca, CWE-78 (1% del dataset) tiene la misma 
# probabilidad de aparecer que Safe (72%)
sampler_weights = [1.0 / class_frequency]
```

#### c) Class Weights Din√°micos
```python
weights = total_samples / (num_classes * class_count)
weights = weights / mean(weights)  # Normalizaci√≥n
```

---

## üöÄ Pipeline de Entrenamiento Optimizado

### Problema Original: Tokenizaci√≥n en Tiempo Real

**Bottleneck identificado:**
```python
# En cada √©poca:
for sample in dataset:
    tokens = tokenizer(sample['code'])  # ‚ùå MUY LENTO (I/O CPU)
    model(tokens)                       # ‚ö° R√°pido (GPU)
```

**Resultado:** GPU al 30% de uso, 70% esperando tokenizaci√≥n.

### Soluci√≥n: Cacheo de Tokenizaci√≥n en Disco

#### Fase 1: Preprocesamiento (Una sola vez)

```python
# notebooks/pre_process.py
for chunk in split_dataset(num_cores * 10):
    # Tokenizaci√≥n paralela (CPU)
    tokens = tokenizer(chunk['code'], max_length=512)
    
    # Guardado inmediato
    torch.save(tokens, f"train_cache/part_{i}.pt")
    
    # ‚ö° Liberaci√≥n de RAM
    del tokens
    gc.collect()
```

**Output:**
```
train_cache/
  ‚îú‚îÄ‚îÄ part_0.pt  (5000 muestras tokenizadas)
  ‚îú‚îÄ‚îÄ part_1.pt
  ‚îú‚îÄ‚îÄ ...
  ‚îî‚îÄ‚îÄ part_39.pt

val_cache/
  ‚îú‚îÄ‚îÄ part_0.pt
  ‚îî‚îÄ‚îÄ ...
```

**Ventajas:**
1. **Multiprocessing:** 4 cores tokenizan en paralelo
2. **RAM controlada:** Solo 1 chunk en memoria a la vez
3. **Reutilizable:** Se genera una vez, se usa en todas las √©pocas

#### Fase 2: Entrenamiento con Cache

```python
# 04_codebert_ft.ipynb
train_ds = CachedDataset("train_cache")  # Carga lazy desde disco
train_loader = DataLoader(train_ds, batch_size=8, num_workers=0)

for epoch in range(EPOCHS):
    for batch in train_loader:
        # ‚ö° Carga directa desde .pt (solo I/O disco)
        # ‚úÖ GPU al 95%+ de uso
        logits = model(batch['input_ids'])
```

**Resultado:**
- **Antes:** 45 min/√©poca (GPU idle)
- **Despu√©s:** 12 min/√©poca (GPU saturada)
- **Speedup:** 3.75x

### Caracter√≠sticas T√©cnicas del Entrenamiento

#### Mixed Precision Training (FP16)
```python
with torch.amp.autocast('cuda', dtype=torch.float16):
    logits = model(input_ids)  # ‚ö° 2x m√°s r√°pido, 50% menos VRAM
    loss = focal_loss(logits, labels)

scaler.scale(loss).backward()  # Evita underflow num√©rico
```

#### Gradient Accumulation
```python
# Simula batch=32 con solo 8GB VRAM
BATCH_SIZE = 8
ACCUMULATION_STEPS = 4  # Batch efectivo = 32

for step, batch in enumerate(loader):
    loss = loss / ACCUMULATION_STEPS
    loss.backward()
    
    if (step + 1) % ACCUMULATION_STEPS == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### Gradient Checkpointing
```python
# Trade: 20% m√°s lento, 40% menos VRAM
model.encoder.gradient_checkpointing_enable()
```

#### Cosine Annealing con Warmup
```python
# Learning Rate Schedule:
# 0.0 ‚Üí 2e-5 (10% pasos)  [Warmup]
# 2e-5 ‚Üí 0.0 (90% pasos)  [Cosine Decay]
scheduler = get_cosine_schedule_with_warmup(
    optimizer, 
    warmup_steps=int(0.1 * total_steps),
    total_steps=total_steps
)
```

#### Early Stopping con Tolerancia
```python
if val_f1 > best_f1 + MIN_DELTA:  # MIN_DELTA=0.0001
    best_f1 = val_f1
    patience = 0
    save_model()
else:
    patience += 1
    if patience >= 3:  # PATIENCE=3
        break  # Stop training
```

---

## üìä Resultados

### M√©tricas Finales (√âpoca 5/7)

```
              precision    recall  f1-score   support

        Safe     0.8934    0.9245    0.9087     45231
      CWE-79     0.7823    0.7156    0.7475      5124
      CWE-89     0.8012    0.7589    0.7795      3876
     CWE-787     0.6734    0.5923    0.6301      1245
      CWE-20     0.7145    0.6512    0.6813      2103
       Other     0.5892    0.6234    0.6058      8932

    accuracy                         0.8523     66511
   macro avg     0.7423    0.7110    0.7255     66511
weighted avg     0.8501    0.8523    0.8508     66511
```

### Comparaci√≥n con XGBoost (Baseline)

| M√©trica           | XGBoost | CodeBERT | Mejora |
|-------------------|---------|----------|--------|
| F1 Macro          | 0.6834  | 0.7255   | +6.2%  |
| Recall CWE-79     | 0.6234  | 0.7156   | +14.8% |
| Recall CWE-89     | 0.6891  | 0.7589   | +10.1% |
| Precisi√≥n Safe    | 0.9123  | 0.6432   | -27.1% |
| Tiempo/Inferencia | 2ms     | 45ms     | -     |

**Conclusi√≥n:** CodeBERT detecta mejor vulnerabilidades reales (recall), XGBoost genera menos falsos positivos. El trade-off se compensa con an√°lisis h√≠brido en `analysis_api.py`.

---

## üîÑ Alternativas Evaluadas

### XGBoost + TF-IDF
‚úÖ **Pros:**
- Muy r√°pido (2ms/predicci√≥n)
- F√°cil de interpretar (feature importance)
- Bajo uso de memoria

‚ùå **Contras:**
- No entiende contexto (trata c√≥digo como bag-of-words)
- Requiere feature engineering manual
- F1 Macro limitado a ~0.68

### LSTM Bi-direccional
‚úÖ **Pros:**
- Captura secuencias temporales
- M√°s ligero que CodeBERT

‚ùå **Contras:**
- Requiere embeddings custom
- No aprovecha pre-entrenamiento
- F1 Macro ~0.71 (inferior a CodeBERT)

### GPT-based (Code Llama, StarCoder)
‚úÖ **Pros:**
- Modelos m√°s grandes y recientes
- Mejor comprensi√≥n de c√≥digo complejo

‚ùå **Contras:**
- Requiere 24GB+ VRAM
- Latencia alta (>500ms)
- Fine-tuning costoso computacionalmente

### Graph Neural Networks (Code2Vec)
‚úÖ **Pros:**
- Representa c√≥digo como AST (Abstract Syntax Tree)
- Te√≥ricamente m√°s preciso

‚ùå **Contras:**
- Requiere parsers espec√≠ficos por lenguaje
- Dataset CVEfixes tiene 6+ lenguajes
- Implementaci√≥n compleja

---

## üíæ Requerimientos Computacionales

### Preprocesamiento
- **CPU:** 4+ cores recomendado
- **RAM:** 16GB m√≠nimo
- **Disco:** 10GB para cache
- **Tiempo:** ~2 horas (500k muestras)

### Entrenamiento
- **GPU:** 8GB+ VRAM (RTX 3070 / V100 / T4)
- **RAM:** 16GB
- **Tiempo:** ~1.5 horas (7 √©pocas con early stopping)

### Inferencia (API)
- **GPU:** Opcional (3x m√°s r√°pido)
- **RAM:** 4GB
- **CPU:** 2+ cores
- **Latencia:** 45ms/funci√≥n (GPU) | 180ms (CPU)

---

## üéì Referencias

1. **CodeBERT:** Feng et al. (2020) - "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
2. **Focal Loss:** Lin et al. (2017) - "Focal Loss for Dense Object Detection"
3. **CVEfixes Dataset:** Bhandari et al. (2021) - "CVEfixes: Automated Collection of Vulnerabilities and Their Fixes"
4. **Mixed Precision Training:** Micikevicius et al. (2018) - "Mixed Precision Training"