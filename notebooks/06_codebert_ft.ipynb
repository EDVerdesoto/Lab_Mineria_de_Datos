{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b40b070",
   "metadata": {},
   "source": [
    "## IMPORTAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.v3.map_cwe import LABEL_NAMES, NUM_LABELS, get_label_id\n",
    "from src.v3.losses import FocalLoss\n",
    "from src.v3.dataset import VulnDataset, collate_fn, CachedDataset\n",
    "from src.v3.model import VulnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8bb98",
   "metadata": {},
   "source": [
    "## CONFIGURAR PARAMETROS PARA EL ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9b2a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "CSV_FILE = \"../data/processed/dataset_ml_ready.csv\"\n",
    "OUTPUT_DIR = \"../models/codebert_vuln/\"\n",
    "MODEL_NAME = \"microsoft/codebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c00906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Épocas y hiperparámetros\n",
    "EPOCHS = 3                  # Aumentado para aprovechar early stopping\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 4      # Batch efectivo = 4 * 8 = 32\n",
    "LEARNING_RATE = 2e-5        # Óptimo para fine-tuning CodeBERT\n",
    "WEIGHT_DECAY = 0.01         # Regularización L2\n",
    "WARMUP_RATIO = 0.1          # 10% de steps para warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4268fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Focal Loss\n",
    "FOCAL_GAMMA = 2.0           # Mayor = más enfoque en ejemplos difíciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80714004",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sliding Window\n",
    "USE_SLIDING_WINDOW = True\n",
    "MAX_LEN = 512\n",
    "STRIDE = 256                # 50% overlap\n",
    "MAX_WINDOWS = 8             # Límite de ventanas por muestra (evita OOM)\n",
    "AGGREGATION = 'max'         # 'max', 'mean', 'attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd367e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "USE_AUGMENTATION = True\n",
    "MASK_PROB = 0.10            # 10% de tokens enmascarados (reducido para ser más conservador)\n",
    "AUGMENT_PROB = 0.3          # 30% de muestras augmentadas (reducido de 0.5)\n",
    "# Variable Renaming está integrado en augmentation (30% de las veces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b255878",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZACIONES\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "#### Early stopping\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARTICIONAMIENTO\n",
    "TEST_SIZE = 0.1\n",
    "NUM_WORKERS = 0\n",
    "SEED = 42\n",
    "CACHE_FILE = \"train_dataset_cached.pt\"\n",
    "\n",
    "### DEVICE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### EARLY STOPPING\n",
    "MIN_DELTA = 0.0001  # Mejora mínima requerida para considerar progreso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26b787",
   "metadata": {},
   "source": [
    "## FUNCIONES DE ENTRENAMIENTO Y AJUSTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ad4409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    weights = [total / (NUM_LABELS * counter.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "    weights = np.array(weights) / np.mean(weights)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efcb1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, scaler, loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    # Usar set_to_none=True es más eficiente que zero_grad() estándar\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        # Mover datos a GPU\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        window_counts = batch['window_counts'].to(DEVICE)\n",
    "        \n",
    "        # --- FORWARD PASS (Mixed Precision) ---\n",
    "        # Usamos torch.amp.autocast (Sintaxis moderna)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            # Pasamos window_counts porque tu modelo VulnClassifier lo requiere\n",
    "            logits = model(input_ids, attention_mask, window_counts)\n",
    "            \n",
    "            # Calculamos Loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Normalizamos la loss para Acumulación de Gradientes\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "        \n",
    "        # --- BACKWARD PASS ---\n",
    "        # Escalamos la loss para evitar underflow en FP16\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Guardamos la loss real (deshaciendo la división) para el reporte\n",
    "        losses.append(loss.item() * ACCUMULATION_STEPS)\n",
    "        \n",
    "        # --- OPTIMIZATION STEP (Solo cada N pasos) ---\n",
    "        if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "            \n",
    "            # 1. Des-escalar gradientes antes de recortarlos (Clipping)\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # 2. Guardar el factor de escala antes del paso\n",
    "            # Esto nos sirve para saber si el paso fue exitoso o se saltó por NaNs\n",
    "            scale_before = scaler.get_scale()\n",
    "            \n",
    "            # 3. Intentar dar el paso del optimizador\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # 4. Obtener el factor de escala nuevo\n",
    "            scale_after = scaler.get_scale()\n",
    "            \n",
    "            # 5. FIX CRÍTICO DEL SCHEDULER:\n",
    "            # Solo avanzamos el Scheduler si el scaler NO redujo la escala.\n",
    "            # (Si scale_after < scale_before, significa que hubo un NaN y el optimizer.step se saltó).\n",
    "            if scale_after >= scale_before:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # 6. Limpiar gradientes\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # 7. Liberar memoria VRAM periódicamente (Opcional pero recomendado con Sliding Window)\n",
    "            if (step + 1) % (ACCUMULATION_STEPS * 50) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # --- METRICAS (Sin gradientes para ahorrar memoria) ---\n",
    "        with torch.no_grad():\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Reporte visual suave\n",
    "        if step % 50 == 0:\n",
    "            avg_loss = np.mean(losses[-50:]) if len(losses) > 0 else losses[-1]\n",
    "            print(f\"\\r  Step {step}/{len(loader)} | Loss: {avg_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    print() # Nueva línea al terminar el epoch\n",
    "    \n",
    "    # Calcular métricas finales del epoch\n",
    "    preds, targs = np.array(all_preds), np.array(all_targets)\n",
    "    return {'loss': np.mean(losses), 'accuracy': (preds == targs).mean(),\n",
    "            'f1_macro': f1_score(targs, preds, average='macro', zero_division=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe671602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses, all_preds, all_targets = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            window_counts = batch['window_counts'].to(DEVICE)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', torch.float16):\n",
    "                logits = model(input_ids, attention_mask, window_counts)\n",
    "                loss = loss_fn(logits, labels)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    preds, targs = np.array(all_preds), np.array(all_targets)\n",
    "    return {'loss': np.mean(losses), 'accuracy': (preds == targs).mean(),\n",
    "            'f1_macro': f1_score(targs, preds, average='macro', zero_division=0)}, targs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4ae4d",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e306478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENTRENAMIENTO - Detección de Vulnerabilidades\n",
      "======================================================================\n",
      "Device: cuda | Batch: 32\n",
      "Sliding Window: True | Augmentation: True\n"
     ]
    }
   ],
   "source": [
    "### INFORMACION DEL ENTRENAMIENTO\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENTRENAMIENTO - Detección de Vulnerabilidades\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE} | Batch: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "print(f\"Sliding Window: {USE_SLIDING_WINDOW} | Augmentation: {USE_AUGMENTATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97b9d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68402f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "1. CARGANDO DATOS\n",
      "======================================================================\n",
      "Total: 181,233\n",
      "  Safe        :  103,945 (57.35%)\n",
      "  CWE-79      :   16,107 ( 8.89%)\n",
      "  CWE-89      :   10,276 ( 5.67%)\n",
      "  CWE-78      :    8,416 ( 4.64%)\n",
      "  CWE-22      :    4,586 ( 2.53%)\n",
      "  CWE-434     :    1,734 ( 0.96%)\n",
      "  CWE-352     :    1,881 ( 1.04%)\n",
      "  Other       :   34,288 (18.92%)\n",
      "\n",
      "Train: 163,109 | Val: 18,124\n"
     ]
    }
   ],
   "source": [
    "### CARGA DE DATOS\n",
    "print(f\"\\n{'=' * 70}\\n1. CARGANDO DATOS\\n{'=' * 70}\")\n",
    "df = pd.read_csv(CSV_FILE, usecols=['code', 'cwe_id']).dropna(subset=['code'])\n",
    "df['label'] = df['cwe_id'].apply(get_label_id)\n",
    "\n",
    "print(f\"Total: {len(df):,}\")\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    c = (df['label'] == i).sum()\n",
    "    print(f\"  {name:12s}: {c:>8,} ({100*c/len(df):5.2f}%)\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=TEST_SIZE, \n",
    "                                        stratify=df['label'], random_state=SEED)\n",
    "print(f\"\\nTrain: {len(train_df):,} | Val: {len(val_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6e864",
   "metadata": {},
   "source": [
    "### DATASET/LOADER ON THE FLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9678a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. DATASETS\n",
      "======================================================================\n",
      "Train dataset: 163109 samples\n",
      "Val dataset: 18124 samples\n",
      "Max windows per sample: 8 (protección contra código muy largo)\n"
     ]
    }
   ],
   "source": [
    "### DATASET/LOADER\n",
    "print(f\"\\n{'=' * 70}\\n2. DATASETS\\n{'=' * 70}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Crear datasets con parámetros configurables\n",
    "train_ds = VulnDataset(\n",
    "    train_df['code'].values, \n",
    "    train_df['label'].values, \n",
    "    tokenizer, \n",
    "    training=True,\n",
    "    use_augmentation=USE_AUGMENTATION,\n",
    "    use_sliding_window=USE_SLIDING_WINDOW,\n",
    "    augment_prob=AUGMENT_PROB,\n",
    "    mask_prob=MASK_PROB,\n",
    "    max_len=MAX_LEN,\n",
    "    stride=STRIDE,\n",
    "    max_windows=MAX_WINDOWS\n",
    ")\n",
    "\n",
    "val_ds = VulnDataset(\n",
    "    val_df['code'].values, \n",
    "    val_df['label'].values, \n",
    "    tokenizer, \n",
    "    training=False,\n",
    "    use_augmentation=False,\n",
    "    use_sliding_window=USE_SLIDING_WINDOW,\n",
    "    max_len=MAX_LEN,\n",
    "    stride=STRIDE,\n",
    "    max_windows=MAX_WINDOWS\n",
    ")\n",
    "\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    counts = Counter(train_df['label'].values)\n",
    "    weights = [1.0 / counts[l] for l in train_df['label'].values]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    shuffle = False\n",
    "else:\n",
    "    sampler, shuffle = None, True\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, BATCH_SIZE, \n",
    "    shuffle=shuffle, \n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, BATCH_SIZE, \n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_ds)} samples\")\n",
    "print(f\"Val dataset: {len(val_ds)} samples\")\n",
    "print(f\"Max windows per sample: {MAX_WINDOWS} (protección contra código muy largo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3916f51",
   "metadata": {},
   "source": [
    "### DATASET/LOADER ON CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET/LOADER\n",
    "print(f\"\\n{'=' * 70}\\n2. DATASETS (HÍBRIDO: Cached Train + Live Val)\\n{'=' * 70}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 1. CARGAR TRAIN DESDE CACHE (Velocidad extrema)\n",
    "# Asumimos que el archivo .pt contiene SOLO los datos de entrenamiento\n",
    "# (o que no te importa entrenar con todo si ya hiciste el split antes)\n",
    "train_ds = CachedDataset(CACHE_FILE)\n",
    "\n",
    "# 2. CARGAR VALIDACIÓN DESDE DATAFRAME (Integridad)\n",
    "# Usamos el split original para validación para asegurarnos de medir bien\n",
    "# Nota: Si tu CACHE_FILE tiene todo el dataset, aquí podrías tener data leakage.\n",
    "# Lo ideal es que CACHE_FILE haya sido creado solo con el split de train.\n",
    "_, val_df_split = train_test_split(df, test_size=TEST_SIZE, \n",
    "                                   stratify=df['label'], random_state=SEED)\n",
    "\n",
    "val_ds = VulnDataset(\n",
    "    val_df_split['code'].values, \n",
    "    val_df_split['label'].values, \n",
    "    tokenizer, \n",
    "    training=False,             # Sin augmentation\n",
    "    use_augmentation=False,\n",
    "    use_sliding_window=USE_SLIDING_WINDOW,\n",
    "    max_len=MAX_LEN,\n",
    "    stride=STRIDE,\n",
    "    max_windows=MAX_WINDOWS\n",
    ")\n",
    "\n",
    "# 3. CONFIGURAR SAMPLER (Adaptado para CachedDataset)\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    print(\"Calculando pesos del sampler sobre datos cacheados...\")\n",
    "    # Extraemos labels directamente de la lista de datos cacheados\n",
    "    # Cada item es un dict {'label': tensor(int), ...}\n",
    "    train_labels = [item['label'].item() if torch.is_tensor(item['label']) else item['label'] \n",
    "                    for item in train_ds.data]\n",
    "    \n",
    "    counts = Counter(train_labels)\n",
    "    # Evitar error si falta alguna clase en el batch\n",
    "    weights = [1.0 / counts.get(l, 1.0) for l in train_labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    shuffle = False\n",
    "else:\n",
    "    sampler, shuffle = None, True\n",
    "\n",
    "# 4. DATALOADERS\n",
    "train_loader = DataLoader(\n",
    "    train_ds, BATCH_SIZE, \n",
    "    shuffle=shuffle, \n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=NUM_WORKERS, # Usar 0 para cached\n",
    "    pin_memory=True          # True ayuda a pasar de RAM a VRAM rápido\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, BATCH_SIZE, \n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=2,           # Validación es pequeña, 2 workers está bien\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset (Cached): {len(train_ds)} samples\")\n",
    "print(f\"Val dataset (Live): {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46abd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3. MODELO\n",
      "======================================================================\n",
      "Modelo cargado: microsoft/codebert-base\n",
      "Agregación: max\n",
      "Número de clases: 8\n"
     ]
    }
   ],
   "source": [
    "### CREACION DEL MODELO\n",
    "print(f\"\\n{'=' * 70}\\n3. MODELO\\n{'=' * 70}\")\n",
    "model = VulnClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    aggregation=AGGREGATION,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    model.encoder.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Modelo cargado: {MODEL_NAME}\")\n",
    "print(f\"Agregación: {AGGREGATION}\")\n",
    "print(f\"Número de clases: {NUM_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67ac4af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "4. CONFIGURANDO ENTRENAMIENTO\n",
      "Memoria GPU limpiada\n",
      "Total steps: 15291\n",
      "Warmup steps: 1529\n"
     ]
    }
   ],
   "source": [
    "### SETUP DEL ENTRENAMIENTO\n",
    "print(f\"\\n{'=' * 70}\\n4. CONFIGURANDO ENTRENAMIENTO\")\n",
    "\n",
    "# Limpiar memoria CUDA antes de empezar\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Memoria GPU limpiada\")\n",
    "\n",
    "class_weights = compute_class_weights(train_df['label'].values)\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=FOCAL_GAMMA)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, \n",
    "                            weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = (len(train_loader) * EPOCHS) // ACCUMULATION_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, int(WARMUP_RATIO * total_steps), total_steps)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {int(WARMUP_RATIO * total_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5. ENTRENAMIENTO\n",
      "======================================================================\n",
      "\n",
      "==================== EPOCH 1/3 ====================\n",
      "  Step 500/20389 | Loss: 1.4615"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_m = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m val_m, y_true, y_pred = evaluate(model, val_loader, loss_fn)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[TRAIN] Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scheduler, scaler, loss_fn)\u001b[39m\n\u001b[32m     30\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Guardamos la loss real (deshaciendo la división) para el reporte\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * ACCUMULATION_STEPS)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# --- OPTIMIZATION STEP (Solo cada N pasos) ---\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % ACCUMULATION_STEPS == \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# 1. Des-escalar gradientes antes de recortarlos (Clipping)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### ENTRENAMIENTO\n",
    "print(f\"\\n{'=' * 70}\\n5. ENTRENAMIENTO\\n{'=' * 70}\")\n",
    "best_f1, patience = 0, 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'=' * 20} EPOCH {epoch + 1}/{EPOCHS} {'=' * 20}\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    train_m = train_epoch(model, train_loader, optimizer, scheduler, scaler, loss_fn)\n",
    "    val_m, y_true, y_pred = evaluate(model, val_loader, loss_fn)\n",
    "    \n",
    "    print(f\"\\n[TRAIN] Loss={train_m['loss']:.4f} Acc={train_m['accuracy']:.4f} F1={train_m['f1_macro']:.4f}\")\n",
    "    print(f\"[VAL]   Loss={val_m['loss']:.4f} Acc={val_m['accuracy']:.4f} F1={val_m['f1_macro']:.4f}\")\n",
    "    print(f\"[TIME]  {(time.time()-t0)/60:.1f}min\")\n",
    "    \n",
    "    history.append({'epoch': epoch+1, 'train': train_m, 'val': val_m})\n",
    "    \n",
    "    if val_m['f1_macro'] > best_f1 + MIN_DELTA:\n",
    "        best_f1, patience = val_m['f1_macro'], 0\n",
    "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/best_model.bin\")\n",
    "        print(f\"\\n>>> MEJOR MODELO! F1={best_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, target_names=LABEL_NAMES, digits=4))\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"\\nPatience: {patience}/{PATIENCE}\")\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\\nFINALIZADO - Mejor F1: {best_f1:.4f}\\n{'=' * 70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
