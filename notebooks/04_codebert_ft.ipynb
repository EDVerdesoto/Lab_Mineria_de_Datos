{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960e1329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu130\n",
      "Requirement already satisfied: torch in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (2.9.1+cu130)\n",
      "Requirement already satisfied: filelock in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch --index-url https://download.pytorch.org/whl/cu130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c5e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.3.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=2.3.5 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.5)\n",
      "Requirement already satisfied: xgboost>=3.1.2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: transformers>=4.57.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 4)) (4.57.3)\n",
      "Requirement already satisfied: matplotlib>=3.10.7 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 5)) (3.10.7)\n",
      "Requirement already satisfied: seaborn>=0.13.2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.7.2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from -r requirements.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from pandas>=2.3.3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from pandas>=2.3.3->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from pandas>=2.3.3->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from xgboost>=3.1.2->-r requirements.txt (line 3)) (2.28.9)\n",
      "Requirement already satisfied: scipy in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from xgboost>=3.1.2->-r requirements.txt (line 3)) (1.16.3)\n",
      "Requirement already satisfied: filelock in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from transformers>=4.57.3->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.57.3->-r requirements.txt (line 4)) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.57.3->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.57.3->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from matplotlib>=3.10.7->-r requirements.txt (line 5)) (3.2.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from scikit-learn>=1.7.2->-r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from scikit-learn>=1.7.2->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=2.3.3->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from requests->transformers>=4.57.3->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from requests->transformers>=4.57.3->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from requests->transformers>=4.57.3->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages (from requests->transformers>=4.57.3->-r requirements.txt (line 4)) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40b070",
   "metadata": {},
   "source": [
    "## IMPORTAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263fd158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dr00p3r/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.v3.map_cwe import LABEL_NAMES, NUM_LABELS, get_label_id\n",
    "from src.v3.losses import FocalLoss\n",
    "from src.v3.dataset import VulnDataset, collate_fn, CachedDataset\n",
    "from src.v3.model import VulnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8bb98",
   "metadata": {},
   "source": [
    "## CONFIGURAR PARAMETROS PARA EL ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b2a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "CSV_FILE = \"../data/processed/dataset_ml_ready.csv\"\n",
    "OUTPUT_DIR = \"../models/codebert_vuln/\"\n",
    "MODEL_NAME = \"microsoft/codebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Épocas y hiperparámetros\n",
    "EPOCHS = 7                  # Aumentado para aprovechar early stopping\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 4      # Batch efectivo = 4 * 8 = 32\n",
    "LEARNING_RATE = 2e-5        # Óptimo para fine-tuning CodeBERT\n",
    "WEIGHT_DECAY = 0.01         # Regularización L2\n",
    "WARMUP_RATIO = 0.1          # 10% de steps para warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4268fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Focal Loss\n",
    "FOCAL_GAMMA = 2.0           # Mayor = más enfoque en ejemplos difíciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80714004",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sliding Window\n",
    "USE_SLIDING_WINDOW = True\n",
    "MAX_LEN = 512\n",
    "STRIDE = 256                # 50% overlap\n",
    "MAX_WINDOWS = 8             # Límite de ventanas por muestra (evita OOM)\n",
    "AGGREGATION = 'max'         # 'max', 'mean', 'attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd367e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "USE_AUGMENTATION = True\n",
    "MASK_PROB = 0.10            # 10% de tokens enmascarados\n",
    "AUGMENT_PROB = 0.3          # 30% de muestras augmentadas\n",
    "# Variable Renaming está integrado en augmentation (30% de las veces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b255878",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZACIONES\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "#### Early stopping\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22b2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARTICIONAMIENTO\n",
    "TEST_SIZE = 0.1\n",
    "NUM_WORKERS = 4\n",
    "SEED = 42\n",
    "CACHE_FILE = \"train_dataset_cached.pt\"\n",
    "\n",
    "### DEVICE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### EARLY STOPPING\n",
    "MIN_DELTA = 0.0001  # Mejora mínima requerida para considerar progreso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26b787",
   "metadata": {},
   "source": [
    "## FUNCIONES DE ENTRENAMIENTO Y AJUSTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad4409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    weights = [total / (NUM_LABELS * counter.get(i, 1)) for i in range(NUM_LABELS)]\n",
    "    weights = np.array(weights) / np.mean(weights)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcb1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, scaler, loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    # Usar set_to_none=True es más eficiente que zero_grad() estándar\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        # Mover datos a GPU\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        window_counts = batch['window_counts'].to(DEVICE)\n",
    "        \n",
    "        # --- FORWARD PASS (Mixed Precision) ---\n",
    "        # Usamos torch.amp.autocast (Sintaxis moderna)\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            # Pasamos window_counts porque tu modelo VulnClassifier lo requiere\n",
    "            logits = model(input_ids, attention_mask, window_counts)\n",
    "            \n",
    "            # Calculamos Loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Normalizamos la loss para Acumulación de Gradientes\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "        \n",
    "        # --- BACKWARD PASS ---\n",
    "        # Escalamos la loss para evitar underflow en FP16\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Guardamos la loss real (deshaciendo la división) para el reporte\n",
    "        losses.append(loss.item() * ACCUMULATION_STEPS)\n",
    "        \n",
    "        # --- OPTIMIZATION STEP (Solo cada N pasos) ---\n",
    "        if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "            \n",
    "            # 1. Des-escalar gradientes antes de recortarlos (Clipping)\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # 2. Guardar el factor de escala antes del paso\n",
    "            # Esto nos sirve para saber si el paso fue exitoso o se saltó por NaNs\n",
    "            scale_before = scaler.get_scale()\n",
    "            \n",
    "            # 3. Intentar dar el paso del optimizador\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # 4. Obtener el factor de escala nuevo\n",
    "            scale_after = scaler.get_scale()\n",
    "            \n",
    "            # 5. FIX CRÍTICO DEL SCHEDULER:\n",
    "            # Solo avanzamos el Scheduler si el scaler NO redujo la escala.\n",
    "            # (Si scale_after < scale_before, significa que hubo un NaN y el optimizer.step se saltó).\n",
    "            if scale_after >= scale_before:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # 6. Limpiar gradientes\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # 7. Liberar memoria VRAM periódicamente (Opcional pero recomendado con Sliding Window)\n",
    "            if (step + 1) % (ACCUMULATION_STEPS * 50) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # --- METRICAS (Sin gradientes para ahorrar memoria) ---\n",
    "        with torch.no_grad():\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Reporte visual suave\n",
    "        if step % 50 == 0:\n",
    "            avg_loss = np.mean(losses[-50:]) if len(losses) > 0 else losses[-1]\n",
    "            print(f\"\\r  Step {step}/{len(loader)} | Loss: {avg_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    print() # Nueva línea al terminar el epoch\n",
    "    \n",
    "    # Calcular métricas finales del epoch\n",
    "    preds, targs = np.array(all_preds), np.array(all_targets)\n",
    "    return {'loss': np.mean(losses), 'accuracy': (preds == targs).mean(),\n",
    "            'f1_macro': f1_score(targs, preds, average='macro', zero_division=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe671602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses, all_preds, all_targets = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            window_counts = batch['window_counts'].to(DEVICE)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', torch.float16):\n",
    "                logits = model(input_ids, attention_mask, window_counts)\n",
    "                loss = loss_fn(logits, labels)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    preds, targs = np.array(all_preds), np.array(all_targets)\n",
    "    return {'loss': np.mean(losses), 'accuracy': (preds == targs).mean(),\n",
    "            'f1_macro': f1_score(targs, preds, average='macro', zero_division=0)}, targs, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4ae4d",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e306478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ENTRENAMIENTO - Detección de Vulnerabilidades\n",
      "======================================================================\n",
      "Device: cuda | Batch: 32\n",
      "Sliding Window: True | Augmentation: True\n"
     ]
    }
   ],
   "source": [
    "### INFORMACION DEL ENTRENAMIENTO\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENTRENAMIENTO - Detección de Vulnerabilidades\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE} | Batch: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "print(f\"Sliding Window: {USE_SLIDING_WINDOW} | Augmentation: {USE_AUGMENTATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b9d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6e864",
   "metadata": {},
   "source": [
    "### DATASET/LOADER ON THE FLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ce2adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CARGANDO DATASETS CACHEADOS (RAM OPTIMIZADA)\n",
      "======================================================================\n",
      "Buscando partes en: train_cache\n",
      "Cargando 40 partes en memoria RAM...\n",
      "Dataset cargado. Total muestras: 163109\n",
      "Buscando partes en: val_cache\n",
      "Cargando 40 partes en memoria RAM...\n",
      "Dataset cargado. Total muestras: 18124\n",
      "Train dataset: 163109 samples\n",
      "Val dataset:   18124 samples\n"
     ]
    }
   ],
   "source": [
    "# 2. CARGA DE DATASETS (Desde Cache)\n",
    "print(f\"\\n{'=' * 70}\\nCARGANDO DATASETS CACHEADOS (RAM OPTIMIZADA)\\n{'=' * 70}\")\n",
    "\n",
    "# Ajusta estas rutas si tus carpetas tienen otro nombre o ubicación\n",
    "TRAIN_CACHE_DIR = \"train_cache\" \n",
    "VAL_CACHE_DIR = \"val_cache\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Cargar datasets\n",
    "train_ds = CachedDataset(TRAIN_CACHE_DIR)\n",
    "val_ds = CachedDataset(VAL_CACHE_DIR)\n",
    "\n",
    "print(f\"Train dataset: {len(train_ds)} samples\")\n",
    "print(f\"Val dataset:   {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afa25579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONFIGURANDO SAMPLER Y PESOS\n",
      "======================================================================\n",
      "Extrayendo etiquetas de entrenamiento...\n",
      "Pesos de clase calculados: tensor([0.0468, 0.3023, 0.4738, 0.5785, 1.0615, 2.8070, 2.5881, 0.1420],\n",
      "       device='cuda:0')\n",
      "Configurando WeightedRandomSampler...\n"
     ]
    }
   ],
   "source": [
    "# 3. CONFIGURACIÓN DE PESOS Y SAMPLER\n",
    "print(f\"\\n{'=' * 70}\\nCONFIGURANDO SAMPLER Y PESOS\\n{'=' * 70}\")\n",
    "\n",
    "# Extraer etiquetas directamente de la memoria (muy rápido)\n",
    "# Necesario porque ya no tenemos el dataframe 'train_df'\n",
    "print(\"Extrayendo etiquetas de entrenamiento...\")\n",
    "train_labels = [sample['label'].item() if torch.is_tensor(sample['label']) else sample['label'] \n",
    "                for sample in train_ds]\n",
    "\n",
    "# Recalcular class_weights para FocalLoss (se usará más adelante en el notebook)\n",
    "class_weights = compute_class_weights(train_labels)\n",
    "print(f\"Pesos de clase calculados: {class_weights}\")\n",
    "\n",
    "# Configurar WeightedRandomSampler\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    print(\"Configurando WeightedRandomSampler...\")\n",
    "    counts = Counter(train_labels)\n",
    "    # Peso inverso a la frecuencia\n",
    "    sampler_weights = [1.0 / counts.get(l, 1.0) for l in train_labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(sampler_weights, len(sampler_weights))\n",
    "    shuffle = False\n",
    "else:\n",
    "    sampler, shuffle = None, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9678a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders listos. ¡A entrenar!\n"
     ]
    }
   ],
   "source": [
    "# 4. DATALOADERS\n",
    "# IMPORTANTE: num_workers=0 es CRÍTICO aquí.\n",
    "# Como los datos ya están en RAM, usar workers > 0 solo añade overhead y lentitud en Windows/Linux.\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=shuffle, \n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=0,      # <--- MANTENER EN 0\n",
    "    pin_memory=True     # Acelera paso RAM -> GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_fn, \n",
    "    num_workers=0,      # <--- MANTENER EN 0\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"DataLoaders listos. ¡A entrenar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284df2c",
   "metadata": {},
   "source": [
    "### MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46abd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3. MODELO\n",
      "======================================================================\n",
      "Modelo cargado: microsoft/codebert-base\n",
      "Agregación: max\n",
      "Número de clases: 8\n"
     ]
    }
   ],
   "source": [
    "### CREACION DEL MODELO\n",
    "print(f\"\\n{'=' * 70}\\n3. MODELO\\n{'=' * 70}\")\n",
    "model = VulnClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    aggregation=AGGREGATION,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    model.encoder.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Modelo cargado: {MODEL_NAME}\")\n",
    "print(f\"Agregación: {AGGREGATION}\")\n",
    "print(f\"Número de clases: {NUM_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac4af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "4. CONFIGURANDO ENTRENAMIENTO\n",
      "Memoria GPU limpiada\n",
      "Extrayendo etiquetas del dataset cacheado para calcular pesos...\n",
      "Total steps: 15291\n",
      "Warmup steps: 1529\n"
     ]
    }
   ],
   "source": [
    "### SETUP DEL ENTRENAMIENTO\n",
    "print(f\"\\n{'=' * 70}\\n4. CONFIGURANDO ENTRENAMIENTO\")\n",
    "\n",
    "# Limpiar memoria CUDA antes de empezar\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Memoria GPU limpiada\")\n",
    "\n",
    "# --- CORRECCIÓN: Extraer etiquetas de train_ds en lugar de train_df ---\n",
    "print(\"Extrayendo etiquetas del dataset cacheado para calcular pesos...\")\n",
    "train_labels = [sample['label'].item() if torch.is_tensor(sample['label']) else sample['label'] \n",
    "                for sample in train_ds]\n",
    "\n",
    "loss_fn = FocalLoss(alpha=None, gamma=FOCAL_GAMMA)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, \n",
    "                            weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = (len(train_loader) * EPOCHS) // ACCUMULATION_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, int(WARMUP_RATIO * total_steps), total_steps)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {int(WARMUP_RATIO * total_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f6cb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5. ENTRENAMIENTO\n",
      "======================================================================\n",
      "\n",
      "==================== EPOCH 1/3 ====================\n",
      "  Step 12800/20389 | Loss: 0.0164"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_m = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m val_m, y_true, y_pred = evaluate(model, val_loader, loss_fn)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[TRAIN] Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_m[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scheduler, scaler, loss_fn)\u001b[39m\n\u001b[32m     26\u001b[39m     loss = loss / ACCUMULATION_STEPS\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# --- BACKWARD PASS ---\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Escalamos la loss para evitar underflow en FP16\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Guardamos la loss real (deshaciendo la división) para el reporte\u001b[39;00m\n\u001b[32m     33\u001b[39m losses.append(loss.item() * ACCUMULATION_STEPS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/College/7th/SecureSoftwareDev/Lab_Mineria_de_Datos/.venv/lib64/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### ENTRENAMIENTO\n",
    "print(f\"\\n{'=' * 70}\\n5. ENTRENAMIENTO\\n{'=' * 70}\")\n",
    "best_f1, patience = 0, 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'=' * 20} EPOCH {epoch + 1}/{EPOCHS} {'=' * 20}\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    train_m = train_epoch(model, train_loader, optimizer, scheduler, scaler, loss_fn)\n",
    "    val_m, y_true, y_pred = evaluate(model, val_loader, loss_fn)\n",
    "    \n",
    "    print(f\"\\n[TRAIN] Loss={train_m['loss']:.4f} Acc={train_m['accuracy']:.4f} F1={train_m['f1_macro']:.4f}\")\n",
    "    print(f\"[VAL]   Loss={val_m['loss']:.4f} Acc={val_m['accuracy']:.4f} F1={val_m['f1_macro']:.4f}\")\n",
    "    print(f\"[TIME]  {(time.time()-t0)/60:.1f}min\")\n",
    "    \n",
    "    history.append({'epoch': epoch+1, 'train': train_m, 'val': val_m})\n",
    "    \n",
    "    if val_m['f1_macro'] > best_f1 + MIN_DELTA:\n",
    "        best_f1, patience = val_m['f1_macro'], 0\n",
    "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/best_model.bin\")\n",
    "        print(f\"\\n>>> MEJOR MODELO! F1={best_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, target_names=LABEL_NAMES, digits=4))\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(f\"\\nPatience: {patience}/{PATIENCE}\")\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\\nFINALIZADO - Mejor F1: {best_f1:.4f}\\n{'=' * 70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
